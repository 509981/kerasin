# Библиотека для автогенерации и оптимизации нейросетей keras.

***Kerasin*** помогает:
 - **Создавать keras-модель под задачу**
 - **Оптимизировать существующую модель**
 - **Проверять качество существующей модели**
 - **Бороться с тараканами** *(это не точно)*
![kerasin_corn](https://user-images.githubusercontent.com/88483677/146186106-6f567ebe-1351-477e-856c-14bd0e1c21e2.png)
Подбор сети происходит с помощью генетического алгоритма. Для этого, kerasin умеет генерировать сети самостоятельно. 
Топология сети случайным образом может образовывать пробросы и ветвления. 
С начала образуется каркас - ориентированный граф с одним входом и одним выходом. Он гарантировано не имеет циклов. 
Затем на этот скелет наращивается "мясо" в виде случайного набора слоев. 
Параметры слоев также меняются случайно, варьируясь в пределах своих допусков.

Как в приличном генетическом алгорите, каждый агент(keras-модель) описывается своим хромосомным набором. Будем называть его геномом. Вот фрагмент такого генома:
```
1.0 MaxPooling1D: name=max_pooling1d_6
1.0 MaxPooling1D: trainable=True
1.0 MaxPooling1D: dtype=float32
1.0 MaxPooling1D: strides=(1,)
1.0 MaxPooling1D: pool_size=(2,)
1.0 MaxPooling1D: inbound_layers=[0]
1.1 Dropout: name=dropout_23
1.1 Dropout: trainable=True
1.1 Dropout: dtype=float32
1.1 Dropout: rate=0.2891937916654299
2.0 Concatenate: name=concatenate_40
2.0 Concatenate: axis=-1
2.0 Concatenate: inbound_layers=[0, 1]
```
Мы можем как считать генотип с керас модели(секвенирование), так и наоборот - синтезировать по геному модель.

Класс поддерживает кроссовер из фрагментов сети своих родителей. Геном родителей режется на две части и одна из них формирует геном потомка. 
По геному можно провести мутацию. Мутации могут касаться как простой смены параметров слоя так и смены типа слоя или их соединения.

Для предотвращения близкородственного скрещивания и вырождения, введено понятие фамилии агента. В кросовере участвуют только агенты с разной фамилией. Новая фамилия присваивается только случайно сгенерированым сетям. Она сохраняется при мутации. При кросовере агент берет фамилию ~~жены~~,ой, ...сети с самой лучшей оценкой.

kerasin называет агента натурально по имени, фамилии и эпохе рождения: **bot_EE.NNN(FFF)**

где: EE - поколение(эпоха рождения); NNN - порядковый номер рождения в эпохе; FFF - фамилия. Например: bot_02.008(013) 

=======

###  Пример
```
!pip install git+https://github.com/509981/Kerasin.git

import numpy as np
from tensorflow.keras import utils
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

import kerasin as ks

# Создаем класс декомпозитора на 5 слоев
G=ks.kerasin(complexity = 1, nPopul=100, maxi_goal=True)

# Описываем параметры модели
G.add_input(shape = (28,28,1))
G.add_output(shape = (10,),layer = Dense(10, activation="softmax"))

# Описываем параметры обучения
G.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
G.ga_control['mutation_prob'] = .3

# Запуск эволюции
G.fit( ga_epochs = 10, x = x_train, y = y_train, batch_size=125, epochs=20, verbose=0, x_val=x_test, y_val = y_test)

# Выводим результат
G.report(False,True)
# Обученные агенты расположены по убыванию val_accuracy. 
utils.plot_model(G.get_model(0), dpi=60, show_shapes=True)
```
---

**Автор будет считать неправомерным использование библиотеки для нужд бюджетных организаций РФ кроме образовательной, здравоохранительной и экологической направленности.**

---
### АТРИБУТЫ КЛАССА

#### Управление генетическим алгоритмом
**ga_control[ <имя параметра> ]**

где, ***<имя параметра>*** - одно из значений:
  
  - *'mutation_prob'* - доля мутации мутанта от исходного генотипа. Например: ga_control['mutation_prob']=.2 (20%)

По умолчанию =  0. При этом значении доля от 0.5 в первой эпохе автоматически уменьшается при приближении к последней эпохе

  - *'popul_distribution'* - распределение популяции. кортеж: (весовая доля оставляемых чемпионов, весовая доля агентов полученых кросовером, весовая доля агентов полученых мутацией, весовая доля случайных агентов). По умолчанию  ga_control['popul_distribution']=(5,25,25,45)

Величина веса не важна, они будут нормированы в пределах всего словаря и приведены к 100%

  - *'early_stopping_at_minloss'* - автоостановка на плато.
Если значение ошибки станет меняться менее чем на величину early_stopping_at_minloss -  прерываем обучение, По умолчанию: 0.005 (0.5%).  Если 0 - отключен

  - *'soft_fit'* - Определяет режим оптимизации. В этом режиме проходят мутации только параметров. Типы слоев и их связи не меняются. Обозначает число, - количество лидеров популяции, которые участвуют в оптимизации. По умолчанию - 0 (отключен)

  - *'autocorrelation_wide'* - количество шагов вперед для оценки автокорреляции(для временных рядов). 
Если на этом количестве шагов будет обнаружена более высокая корреляция, чем на 0-ом шаге,- сеть будет отбракована. 0 - по умолчанию - функция отключена

---
### МЕТОДЫ КЛАССА

#### Конструктор класса
**kerasin( complexity = 1, nPopul=10, maxi_goal=False )**
   
  - *complexity* - сложность сети 1 - на 5 слоев, 2-на 10 и т.п. Количество слоев при генерации агентов точно не соблюдается.
  - *nPopul* - размер популяции
  - *maxi_goal* - целевая функция метрики. True,- если максимизация(accuracy), False - минимизация(loss)

Конструкция создает экземпляр класса 
  
---
#### Подмешивание в популяцию внешних моделей
**add_model( model, name='' )**
 - *model* - керас модель
 - *name* - имя агента

Для ускорения сходимости или оптимизации конкретной модели, описываем в Sequential или функциональной форме keras и добавляем ее в популяцию. Следите чтобы имя агента было уникальным. Возвращает True в случае успеха.

---
#### Генерация случайных агентов.  
**generate( nPopul=1, epoch=0 )**
  - *nPopul* - количество агентов для генерации
  - *epoch* - Текущая эпоха. Задание параметра epoch важно, для правильного именования агентов.

Генерация nPopul случайных агентов.  

---
#### Описание входа моделей. 
**add_input( shape, isSequence = False, maxWordCount = 0)**
  - *shape* - форма входа(Без batch размерности) для mnist это может быть (28,28) или (784,)
  - *isSequence* - указание на то, что в подаваемых данных важна последовательность
  - *maxWordCount* - Количество слов в словаре Embedding. maxWordCount больше нуля - указание на то, что необходимо использовать слой Embedding с указаным количеством слов в словаре. В этом случае следует подумать о значении *isSequence* 

Добавить описание входа моделей. 

---
#### Описание выхода моделей. 
**add_output( shape, layer )**
  - *shape* - форма выхода. 
  - *layer* - выходной слой керас

Добавить описание выхода моделей. 

---
#### Установка параметров обучения популяции
**compile( optimizer="adam", loss=None, metrics=None )**

Описываем параметры обучения популяции. Назначение параметров совпадает с их назначением в keras описанных [здесь](https://keras.io/api/models/model_training_apis/). Однако metrics не может быть списком, только строкой. Например: metrics='mae'

---
#### Запуск эволюции
**fit( ga_epochs=1, x=None, y=None, batch_size=None,  epochs=1,  verbose="auto",  validation_split=0.0,  x_val=None,  y_val=None, rescore = False )**
  - *ga_epochs* - количество эпох генетики, Не путать с *epoch* - количеством эпох обучения модели
  - *rescore* - принудительная перетренировка ранее тренированных моделей.

Запуск эволюции. Назначение остальных параметров совпадает с их назначением в keras описанных [здесь](https://keras.io/api/models/model_training_apis/)
Если заданное количество ga_epoch уже достигнуто ранее. Повторный вызов приведет только к загрузке популяции из профиля и к оценке неоцененных агентов.

---
#### Запуск эволюции c генератором
**fit_generator( ga_epochs=1, train_gen = None, batch_size=None, epochs=1, verbose="auto", validation_gen = None, rescore = False )**
  - *ga_epochs* - количество эпох генетики. Не путать с *epoch* - количеством эпох обучения модели
  - *rescore* - принудительная тренировка моделей которые уже прошли оценку.

Запуск эволюции на ga_epochs генетических эпох c генератором. Назначение остальных параметров совпадает с их назначением в keras. Все описано [здесь](https://keras.io/api/models/model_training_apis/)
Если заданное количество ga_epoch уже достигнуто ранее. Повторный вызов приведет только к загрузке популяции из профиля и к оценке неоцененных агентов.

---
#### Получить керас модель  
**get_model( idx=0 )**
  - *idx* - индекс агента в популяции

Получить керас модель от idx агента популяции. Все агенты пронумерованы в порядке убывания оценки т.е. победитель имеет индекс - 0

---
#### Отчет по эпохе
**report( scoreboard=True, best_detail=False ,pSurv=.5)**
  - *scoreboard* - Вывод сводной таблицы чемпионов
  - *best_detail* - добавляем подробное описание чемпиона
  - *pSurv* - Коэффициент выводимых в таблице чемпионов. Если 1 в scoreboard попадет вся популяция(100)%. По умолчанию 0,5 (50%).

Отчет по эпохе

---
#### Получить индекс агента по имени
**get_index( bot_name )**
  - *bot_name* - имя агента

Получаем индекс агента по имени в популяции. -1 если такого агента в текущей популяции нет
Так-же не забываем, что индекс это место на скамеейке победителей

---
#### Получить имя агента по индексу
**get_bot_name( bot_idx = 0 )**
  - *bot_idx* - индекс агента в популяции

Возвращает имя агента по индексу в текущей популяции. Если параметр не задан, получаем имя чемпиона

---
#### Получить керас-код агента
**print_code( idx=0, parameter_limit=0 ,valname='x' )**
  - *bot_idx* - индекс агента в популяции
  - *limit* - Вывод первыx limit параметров функции. 0 - все
  - *valname* - Имя переменной для подстановки в код

Получение кода сети на Питон в функциональной нотации керас. Генерируемый исходный код может потребовать доработки. Ветвления в сети правильно не обрабатываются. 

---
#### Установка вероятности выпадения основных слоев
**set_layers_type_prob( new_layers_type_prob )**
  - *new_layers_type_prob* - словарь с поддерживаемым составом основных слоев керас из списка и их вес в выпадении при генерации сети

По умолчанию установлено значение: ***{'Dense':5, 'Conv2D':5, 'Conv1D':5, 'MaxPooling1D':2, 'MaxPooling2D':2,
                    'LSTM':5, 'GRU':4, 'SimpleRNN':2, 'Flatten':3, 'RepeatVector':2, 'GaussianNoise':1, 
                    'GlobalMaxPooling1D': 1, 'GlobalMaxPooling2D': 1}***
Список соответствует списку поддерживаемых основных слоев. Указание других слоев - не допускается. Список будет расширяться. Для управление вспомогательными слоями Dropout,BatchNormalization см. **set_extralayers_type_prob()**. 

---

#### Установка вероятности выпадения вспомогательных слоев
**set_extralayers_type_prob( layer, prob )**
  - *layer* - имя вспомогательного слоя
  - *prob* - вероятность в пределах [0,1)
	
Устанавливаем вероятность prob выпадения layer после основного слоя при генерации сети. По умолчанию вероятность *Dropout* : 0.3; *BatchNormalization*: 0.3; *LeakyReLU*: 0.15.
Для управление основными слоями см. **set_layers_type_prob()**. В отличии от set_extralayers_type_prob() мы указываем вероятность а не вес. 

---

## Несколько эмпирических правил

### Создание сети

  - Лучше большая популяция, чем глубокая эволюция.
  - Не снижать размерность входных данных без необходимости - Чем больше размерность подаваемых данных, тем богаче инструментарий слоев.
  - Количество эпох обучения не имеет существенного значения. Практика пока не показала, что среди аутсайдеров на короткой дистанции есть победители на длинной
  - Если весь пьедестал заняли родственники. Останавливайте эволюцию.
  - Если победитель на сложной модели нашелся быстро. Стоит повторить эволюцию с начала снизив сложность(complexity). Возможно вы имеете дело с переобучением.
  - Нельзя пренебрегать остальными моделями – призерами. Из них предпочитать более простые.

### Оптимизация сети
 - Оптимизацию лучше делать при небольшой силе мутаций
 - Допускайте в оптимизацию не одного победителя

### Посев
 - Полезно повторять эволюцию с начала подмешивая победителей параллельных эволюций
 - Не стоит подмешивать посев  на поздних стадиях генетических итераций. Потеряете время, а посев не оставит следа.
